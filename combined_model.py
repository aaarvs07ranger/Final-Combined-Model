# -*- coding: utf-8 -*-
"""Combined Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zRGyjmOVCQIHsMJwmbikyyRFGM4eMXLJ
"""

import os
import numpy as np
import pandas as pd
import dlib
import cv2
import torchaudio
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Concatenate, Input
from sklearn.model_selection import train_test_split
from imutils import face_utils
import matplotlib.pyplot as plt
from sklearn import metrics
!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2

def extract_landmarks(image_path):
    if not isinstance(image_path, str) or image_path.strip() == "":
        return np.zeros(136)
    image = cv2.imread(image_path)
    if image is None:
        return np.zeros(136)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
    rects = detector(gray, 1)
    for (i, rect) in enumerate(rects):
        shape = predictor(gray, rect)
        shape = face_utils.shape_to_np(shape)
        return shape.flatten()
    return np.zeros(136)

def extract_mfcc(audio_path, desired_length=50):
    waveform, sample_rate = torchaudio.load(audio_path)
    mfcc_transform = torchaudio.transforms.MFCC(sample_rate=sample_rate)
    mfcc = mfcc_transform(waveform)
    if mfcc.shape[2] < desired_length:
        # Pad if the MFCC output is shorter than the desired length
      padding = desired_length - mfcc.shape[2]
      mfcc = torch.nn.functional.pad(mfcc, (0, padding), "constant", 0)
    elif mfcc.shape[2] > desired_length:
        # Truncate if MFCC output is longer than desired length
      mfcc = mfcc[:, :, :desired_length]

    return mfcc

df = pd.read_excel('Split up audios spreadsheet (2).xlsx')
df['landmarks'] = df['Image File Name'].apply(extract_landmarks)
desired_mfcc_length = 50
df['mfcc'] = df['Audio File Name'].apply(lambda x: extract_mfcc(f'Split dataset audios/{x}.wav', desired_length=desired_mfcc_length))

truth_or_lie_labels = pd.get_dummies(df['Veracity']).values
X_cv = np.array(df['landmarks'].tolist())
X_speech = np.array(df['mfcc'].tolist())

min_mfcc_length = min(sample.shape[2] for sample in X_speech)
X_speech = np.array([sample[:, :, :min_mfcc_length] for sample in X_speech], dtype=np.float32)

x_cv_train, x_cv_test, x_speech_train, x_speech_test, y_train, y_test = train_test_split(
    X_cv, X_speech, truth_or_lie_labels, test_size=0.5, random_state=42)

cv_input = Input(shape=(136, 1))
cv_model = Sequential()
cv_model.add(Conv1D(32, kernel_size=(2), activation='relu', input_shape=(136, 1)))
cv_model.add(MaxPooling1D(pool_size=(2)))
cv_model.add(Conv1D(64, kernel_size=(2), activation='relu'))
cv_model.add(MaxPooling1D(pool_size=(2)))
cv_model.add(Flatten())
cv_model.add(Dense(512, activation='relu', input_shape=(136, 1)))
cv_model.add(BatchNormalization())
cv_model.add(Dropout(0.5))
cv_model.add(Dense(256, activation='relu'))
cv_model.add(BatchNormalization())
cv_model.add(Dropout(0.5))
cv_model.add(Dense(128, activation='relu'))
cv_model.add(BatchNormalization())
cv_model.add(Dropout(0.5))
cv_features = cv_model(cv_input)

mfcc_input = Input(shape=(2, X_speech.shape[2], X_speech.shape[3]))
speech_model = Sequential()
speech_model.add(Flatten())
speech_model.add(Dense(512, activation='relu'))
speech_model.add(BatchNormalization())
speech_model.add(Dropout(0.5))
speech_model.add(Dense(256, activation='relu'))
speech_model.add(BatchNormalization())
speech_model.add(Dropout(0.5))
speech_features = speech_model(mfcc_input)

combined_features = Concatenate()([cv_features, speech_features])
x = Dense(512, activation='relu')(combined_features)
x = Dropout(0.5)(x)
x = Dense(256, activation='relfu')(x)
x = Dropout(0.5)(x)
output = Dense(2, activation='softmax')(x)

final_model = Model(inputs=[cv_input, mfcc_input], outputs=output)
final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),
                    loss='categorical_crossentropy',
                    metrics=['accuracy'])

history = final_model.fit([x_cv_train, x_speech_train], y_train, epochs=20, validation_data=([x_cv_test, x_speech_test], y_test))

test_loss, test_accuracy = final_model.evaluate([x_cv_test, x_speech_test], y_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

y_pred = final_model.predict([x_cv_test, x_speech_test])
confusion_matrix = metrics.confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[0, 1])
cm_display.plot()
plt.show()